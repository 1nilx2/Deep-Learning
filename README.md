# Statistics

## Cross-entropy & KL Divergence
Entropy is average amount of information and can be said degree of surprise

for discrete r.v. x,

$\sum_{x}{p(x)log_{b}{p(x)}} = E_p[-log_{b}{p(x)}]$
