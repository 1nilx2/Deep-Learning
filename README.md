# Statistics

## Cross-entropy & KL Divergence
Entropy is average amount of information and can be said degree of surprise


$$
\displaylines{\mathit{for \ discrete \ x,} \\ 
\sum_{x}{p(x)log_{b}{p(x)}} = E_p[-log_{b}{p(x)}]}
$$
