# Statistics

## Cross-entropy & KL Divergence
Entropy is average amount of information and can be said degree of surprise

$$
\mathit{for \ discrete \ x,} \\\ 
\sum_{x}{p(x)log_{b}{p(x)}} = E_p[-log_{b}{p(x)}]
$$

$$
\displaylines{\mathit{for \ continuous \ x,} \\ 
\sum_{x}{p(x)log_{b}{p(x)}} = E_p[-log_{b}{p(x)}]}
$$
