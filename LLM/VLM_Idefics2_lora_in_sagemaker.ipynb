{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5d4a570c-c4b1-4d74-8a87-60350988fbee",
      "metadata": {
        "id": "5d4a570c-c4b1-4d74-8a87-60350988fbee"
      },
      "source": [
        "# [HF Reference](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Idefics2/Fine_tune_Idefics2_for_JSON_extraction_use_cases_(PyTorch_Lightning).ipynb)\n",
        "- In this notebook, we'll use `LoRA` instead of `QLoRA`.\n",
        "- Due to VRAM usage, running this notebook as-is requires A100 GPU\n",
        "    - You might be able to run this in an AWS Instace like `ml.g5.2xlarge 24GB VRAM`, if you adopt additional tactics to reduce memory usage such as `graident_checkpointing` or use `QLoRA`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q accelerate datasets peft bitsandbytes\n",
        "!pip install wandb -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPxt2Ib-aHE",
        "outputId": "0af1e05e-2093-43ea-9e79-a274923a6d30",
        "collapsed": true
      },
      "id": "dHPxt2Ib-aHE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade peft\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gLuSdN8AV8x",
        "outputId": "5fddb165-4ea8-4527-e4c1-79a738b224f4",
        "collapsed": true
      },
      "id": "_gLuSdN8AV8x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.45.0.dev0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.32.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.6.20)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.45.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f22a173-7dd9-44d6-88e1-66ee765c0a1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f22a173-7dd9-44d6-88e1-66ee765c0a1f",
        "outputId": "459b928f-5723-421b-935c-702567cb4410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of GPUs available: 1\n",
            "GPU 0: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    print(\"No GPU available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8d4273-227e-4a5c-b02d-7aefb881254b",
      "metadata": {
        "scrolled": true,
        "id": "4d8d4273-227e-4a5c-b02d-7aefb881254b"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics2ForConditionalGeneration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b39a41-746e-4014-a431-18eec148f939",
      "metadata": {
        "id": "59b39a41-746e-4014-a431-18eec148f939"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda:0\"\n",
        "USE_LORA = True\n",
        "USE_QLORA = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm gonna use different model accroding to the type of data. If a dataset requires more lengthy conversation, I'm going to use a model tuned for that purpose. You can choose a variety of models in Hugging Face. What matters is to prepare your own datset and adjust `data collator`"
      ],
      "metadata": {
        "id": "DONdxLIGPmse"
      },
      "id": "DONdxLIGPmse"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c4921ec-01e1-46a7-a009-8a0a1abc82a5",
      "metadata": {
        "id": "4c4921ec-01e1-46a7-a009-8a0a1abc82a5"
      },
      "outputs": [],
      "source": [
        "# v1: multiple queries per image for each item\n",
        "# v2: per image, only one query which includes all the items such as part_number, materials, ...\n",
        "dataset_version = \"v1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6d81e20-6315-4ec5-8f4e-df5e478001f1",
      "metadata": {
        "id": "d6d81e20-6315-4ec5-8f4e-df5e478001f1"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "879b4d13-bc7f-40f8-8fc8-d6b77c891b40",
      "metadata": {
        "id": "879b4d13-bc7f-40f8-8fc8-d6b77c891b40"
      },
      "outputs": [],
      "source": [
        "# 8b-chatty is tuned further from 8b to improve the ability in long sequence\n",
        "model_name = \"HuggingFaceM4/idefics2-8b\" if dataset_version == \"v1\" else \"HuggingFaceM4/idefics2-8b-chatty\","
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "572cd1c6-dffa-49d8-ad4f-11df8886cabe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "572cd1c6-dffa-49d8-ad4f-11df8886cabe",
        "outputId": "2d9ed0ed-0094-4a36-c36d-9df38d736895",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Chat templates should be in a 'chat_template.json' file but found key='chat_template' in the processor's config. Make sure to move your template to its own file.\n"
          ]
        }
      ],
      "source": [
        "# Processor prepares your data to be transferred to model\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_name[0],\n",
        "    do_image_splitting=False  # Concatenates the original and the 4 pieces of splitted images\n",
        "                              # Activate for tasks involving sophisticated OCR, and thus higher resolution\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b832a0-50aa-4196-aaeb-683e53791612",
      "metadata": {
        "id": "30b832a0-50aa-4196-aaeb-683e53791612"
      },
      "outputs": [],
      "source": [
        "args_lora = dict(\n",
        "    r=8,               # Rank. Decomposition level\n",
        "    lora_alpha=8,      # Scale. Extent of LoRA adapter to original weights\n",
        "    lora_dropout=0.1,\n",
        "    # Modules approximiated by LoRA\n",
        "    target_modules='.*(text_model|modality_projection|perceiver_resampler).*(down_proj|gate_proj|up_proj|k_proj|q_proj|v_proj|o_proj).*$',\n",
        "    use_dora=False if USE_QLORA else True,\n",
        "    init_lora_weights=\"gaussian\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83c30c4a-191a-451d-8a89-515119bd0b26",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9573195f40bc4f239fa9cb00e07da3bf",
            "9669b8d294b9470e92c114e388aac3c0",
            "aebc8e4a6e2d43af9f2e9ee6f6b75c6d",
            "b0f900e9188b4e76bec2274f028348f0",
            "3bd9a30262b543fba290b1fc1e4b0fc6",
            "7ec0ebcd886b40ca92cd7408a9c806dd",
            "39039a99a96f475d9f8662c42f5abe4c",
            "1e415c5fda1b4c6291c45f15cb8a9577",
            "1e34149c1fa44891a9d0dd2015ea4e8b",
            "4827edb5b36946a9870916b51aefa154",
            "d6f123b9b13d452881ce6a80257c8799"
          ]
        },
        "id": "83c30c4a-191a-451d-8a89-515119bd0b26",
        "outputId": "b6f7fd28-faed-4767-b1a4-6d8d52f4873a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9573195f40bc4f239fa9cb00e07da3bf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from peft import get_peft_model\n",
        "\n",
        "if USE_QLORA or USE_LORA:\n",
        "    lora_config = LoraConfig(**args_lora)\n",
        "    if USE_QLORA:\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16\n",
        "        )\n",
        "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
        "        model_name[0],\n",
        "        torch_dtype=torch.float16,\n",
        "        quantization_config=bnb_config if USE_QLORA else None,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "else:\n",
        "    model = Idefics2ForConditionalGeneration.from_pretrained(\n",
        "        model_name[0],\n",
        "        torch_dtype=torch.float16,\n",
        "        _attn_implementation=\"flash_attention_2\", # This works for A100 or H100\n",
        "    ).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3f8d68-e94e-4e2f-ba91-58821e4bfdb3",
      "metadata": {
        "id": "8d3f8d68-e94e-4e2f-ba91-58821e4bfdb3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "train_dataset = load_dataset(f\"nilx21/VLM_benchmark_{dataset_version}\", split=\"train\", token=\"<your_token_key>\")\n",
        "eval_dataset = load_dataset(f\"nilx21/VLM_benchmark_{dataset_version}\", split=\"test\", token=\"<your_token_key>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0ba0f5-ce9c-4f13-8273-e6d20c4e0e17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b0ba0f5-ce9c-4f13-8273-e6d20c4e0e17",
        "outputId": "93d88aa6-2f08-42bf-a838-02ea3aa3c804"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1970"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5fc8322-4979-4d7a-9539-f80feaa0d1cf",
      "metadata": {
        "id": "e5fc8322-4979-4d7a-9539-f80feaa0d1cf"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# DataCollator\n",
        "class MyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "        self.image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
        "            processor.tokenizer.additional_special_tokens.index(\"<image>\")\n",
        "        ]\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        texts = []\n",
        "        images = []\n",
        "        for example in examples:\n",
        "            image = Image.open(io.BytesIO(example[\"image\"]))   # PIL.JPEG Image. Does mine match with this?\n",
        "            question = example[\"query\"]  # Change this part. Maybe I need to change the format of dataset little bit\n",
        "            answer = example[\"answer\"]  # Answer is given as a list of possible answers.\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        # {\"type\": \"text\", \"text\": \"Answer briefly.\"},\n",
        "                        {\"type\": \"text\", \"text\": \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\"},\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": answer}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "            text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
        "            texts.append(text.strip())\n",
        "            images.append([image])\n",
        "\n",
        "        batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        labels = batch[\"input_ids\"].clone()\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = self.image_token_id\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# DataCollator\n",
        "data_collator = MyDataCollator(processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b61006d7-6dc0-4daa-9b37-16b988ec55c5",
      "metadata": {
        "id": "b61006d7-6dc0-4daa-9b37-16b988ec55c5"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2111c17c-9e24-467d-b895-01c477848ae7",
      "metadata": {
        "id": "2111c17c-9e24-467d-b895-01c477848ae7"
      },
      "source": [
        "## Trainer arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d68fe3-5f26-41b1-9cef-ccb3d65a94b7",
      "metadata": {
        "id": "c1d68fe3-5f26-41b1-9cef-ccb3d65a94b7"
      },
      "outputs": [],
      "source": [
        "eps = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf3abb2-188e-4669-b056-4386bf77db11",
      "metadata": {
        "id": "2cf3abb2-188e-4669-b056-4386bf77db11"
      },
      "outputs": [],
      "source": [
        "args = dict(\n",
        "    num_train_epochs=eps,\n",
        "    # max_steps=60,  # If both train_epochs and steps exist, steps override.\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # => Pseudo-Batch = B_per_device * num_device * accumulation_step\n",
        "    # gradient_checkpointing=True,  # This is not compatible base setting of `model.config.use_cache=True`\n",
        "    warmup_steps=50,\n",
        "    learning_rate=1.0e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=5,\n",
        "    output_dir=f\"/contents\",  #adapters/adapter_{dataset_version}_cropped_eps{eps}_steps0\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    # eval_strategy=\"steps\",\n",
        "    # eval_steps=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=300,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    # report_to=\"none\",\n",
        "    report_to=\"wandb\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bac141f-a3e5-4a73-aba8-6289c76bdc0b",
      "metadata": {
        "id": "3bac141f-a3e5-4a73-aba8-6289c76bdc0b"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(**args)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be16f3ec-9e01-42b6-97f7-f144214604c7",
      "metadata": {
        "id": "be16f3ec-9e01-42b6-97f7-f144214604c7"
      },
      "source": [
        "### Supplement args to be reported to Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f40b58-f54c-432e-bab7-0ebd5d9e4645",
      "metadata": {
        "id": "e0f40b58-f54c-432e-bab7-0ebd5d9e4645"
      },
      "outputs": [],
      "source": [
        "# Wandb setting\n",
        "eps = 0 if args.get('max_steps') else args['num_train_epochs']\n",
        "steps = args.get('max_steps') if args.get('max_steps') else 0\n",
        "\n",
        "user = None  #\n",
        "project = '<your_project_name>'\n",
        "display_name = \"<run_name>\"\n",
        "print(f\"run name: {project}/{display_name}\")\n",
        "\n",
        "# As this items not included in training_arguments\n",
        "args['use_lora'] = USE_LORA\n",
        "args['use_qlora'] = USE_QLORA,\n",
        "\n",
        "args['lora_config'] = lora_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4d4b520-5e5e-43a0-a860-6ac8b04841c1",
      "metadata": {
        "scrolled": true,
        "id": "d4d4b520-5e5e-43a0-a860-6ac8b04841c1"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "# wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891a585e-a9a8-44c3-8897-076d3dee82ba",
      "metadata": {
        "id": "891a585e-a9a8-44c3-8897-076d3dee82ba"
      },
      "outputs": [],
      "source": [
        "!wandb login '<your_key>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0795954c-1fc5-4442-9681-295ab17fa24d",
      "metadata": {
        "id": "0795954c-1fc5-4442-9681-295ab17fa24d"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "951a5b69-409a-48ee-869e-bc4a4cbd8270",
      "metadata": {
        "id": "951a5b69-409a-48ee-869e-bc4a4cbd8270"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=project, name=display_name, config=args, reinit=True)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a700730d-0ad2-4c42-aa5b-c4819e576856",
      "metadata": {
        "id": "a700730d-0ad2-4c42-aa5b-c4819e576856"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f469a98e-6e7b-405b-8699-368188753af3",
      "metadata": {
        "id": "f469a98e-6e7b-405b-8699-368188753af3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9573195f40bc4f239fa9cb00e07da3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9669b8d294b9470e92c114e388aac3c0",
              "IPY_MODEL_aebc8e4a6e2d43af9f2e9ee6f6b75c6d",
              "IPY_MODEL_b0f900e9188b4e76bec2274f028348f0"
            ],
            "layout": "IPY_MODEL_3bd9a30262b543fba290b1fc1e4b0fc6"
          }
        },
        "9669b8d294b9470e92c114e388aac3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec0ebcd886b40ca92cd7408a9c806dd",
            "placeholder": "​",
            "style": "IPY_MODEL_39039a99a96f475d9f8662c42f5abe4c",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "aebc8e4a6e2d43af9f2e9ee6f6b75c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e415c5fda1b4c6291c45f15cb8a9577",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e34149c1fa44891a9d0dd2015ea4e8b",
            "value": 7
          }
        },
        "b0f900e9188b4e76bec2274f028348f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4827edb5b36946a9870916b51aefa154",
            "placeholder": "​",
            "style": "IPY_MODEL_d6f123b9b13d452881ce6a80257c8799",
            "value": " 7/7 [00:06&lt;00:00,  1.08it/s]"
          }
        },
        "3bd9a30262b543fba290b1fc1e4b0fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec0ebcd886b40ca92cd7408a9c806dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39039a99a96f475d9f8662c42f5abe4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e415c5fda1b4c6291c45f15cb8a9577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e34149c1fa44891a9d0dd2015ea4e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4827edb5b36946a9870916b51aefa154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6f123b9b13d452881ce6a80257c8799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}