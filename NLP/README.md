# Transformer

## Background
Recurrent model computes 'sequentially'. This shown difficulty in parallelization, decreased performance with long sequence, and limitation of batch due to memory usage (Some remedies exist like factorization tricks and conditional compution).

Transformer and, thus, attention mechanism has been suggested to overcome these problems

## Seq2Seq
![seq2seq](https://user-images.githubusercontent.com/88100984/227757491-94779534-447e-40bf-aa64-c332c675d398.jpg)

## Note
![Transformer_1](https://user-images.githubusercontent.com/88100984/227757504-f1743c32-049b-4377-becf-a6c062bfa41c.JPG)

![Transformer-2](https://user-images.githubusercontent.com/88100984/228073192-8fde7f83-3f0c-4b59-80c9-3e9c7421b3ad.jpg)



ref: https://happy-jihye.github.io/nlp/
